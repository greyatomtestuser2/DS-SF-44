# DS-SF-44 Pre-work Summary

Class | Pre-work (be able to...)
--- | ---
3/20: Intro to Data Science | - [Define basic data types used in object-oriented programming](https://www.computerhope.com/jargon/d/datatype.htm) <br /> - [Recall the Python syntax for lists, dictionaries, and functions](http://thomas-cokelaer.info/tutorials/python/data_structures.html) <br /> - [Create files and navigate directories using the command line interface](https://www.codecademy.com/courses/learn-the-command-line/lessons/navigation/exercises/your-first-command)
3/22: Numpy & Pandas | - Can [create and open an Jupyter Notebook](https://unidata.github.io/online-python-training/notebook.html) <br /> - [Completed the Python pre-work](./ds-prework-student.md)
3/27: Statistics Fundamentals I | 
3/29: Statistics Fundamentals II | - Explain the [difference between variance and bias](http://scott.fortmann-roe.com/docs/BiasVariance.html) <br /> - Use [descriptive statistics](https://machinelearningmastery.com/understand-machine-learning-data-descriptive-statistics-python/) to understand your data
4/3: Flex Session |
4/5: Introduction to Linear Regression | - Effectively [show correlations between an independent variable x and a dependent variable y](http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Correlation-Regression/BS704_Correlation-Regression_print.html) <br /> - Be familiar with the [get_dummies function in pandas](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) <br /> -Understand the [difference between vectors, matrices, Series, and DataFrames](https://pandas.pydata.org/pandas-docs/stable/dsintro.html) <br /> - Understand the concepts of [outliers and distance](https://machinelearningmastery.com/how-to-identify-outliers-in-your-data/) <br /> - Be able to interpret [p values and confidence intervals](http://blog.minitab.com/blog/adventures-in-statistics-2/understanding-hypothesis-tests%3A-confidence-intervals-and-confidence-levels)
4/10: Evaluating Model Fit | - [Understand goodness of fit (r-squared)](http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit) <br /> - [Measure statistical significance of features](http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients) <br /> - [Recall what a residual is](http://stattrek.com/regression/residual-analysis.aspx?Tutorial=AP) <br /> - [Implement a sklearn estimator to predict a target variable](http://scikit-learn.org/stable/tutorial/statistical_inference/settings.html)
4/12: Intro to classification | - [Understand how to optimize for error in a model](https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/) <br /> - [Understand the concept of iteration to solve problems](https://www.pythonlearn.com/html-008/cfbook006.html) <br /> - [Measure basic probability](https://towardsdatascience.com/basic-probability-theory-and-statistics-3105ab637213)
4/17: Intro to Logistic Regression | - [Implement a linear model (LinearRegression) with sklearn or statsmodels](http://bigdata-madesimple.com/how-to-run-linear-regression-in-python-scikit-learn/) <br /> - [Define the concept of coefficients](http://statisticsbyjim.com/glossary/regression-coefficient/) <br /> - [Recall metrics for accuracy and misclassification](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) <br /> - [Recall the differences between L1 and L2 regularization](https://www.r-bloggers.com/machine-learning-explained-regularization/)
4/19: Communicating the Results of Logistic Regression | - Understand results from a confusion matrix, and measure true positive rate and false positive rate <br /> - Create and interpret results from a binary classification problem <br /> - Know what a decision line is in logistic regression
4/24: Clustering | - Use sckit-learn to fit models <br /> - Use seaborn to create plots <br /> - Use pandas to load datasets
4/26: Decision Trees | - Use seaborn to create plots <br /> - Knowledge of a bootstrap sample <br /> - Explain the concepts of cross-validation, logistic regression, and overfitting <br /> - Know how to build and evaluate some classification model in sckit-learn using cross-validation and AUC
5/1: NLP with Classification | - Experience with sckit-learn classifiers, specifically Random Forests and Decision trees <br /> - Install spacy with pip install spacy (or use Anaconda) <br /> - Run the spacy download data command
5/3: Latent Variable Models | - Install gensim with pip install gensim <br /> - Recall and apply unsupervised learning techniques <br /> - Recall probability distributions, specifically discrete multinomial distributions <br /> - Recall NLP essentials, including experience with spacy <br /> - BONUS: If you are interested in accessing the Twitter API, you'll need to [setup Twitter API credentials](./twitter-instructions.md)
5/8: Time-series Data | - Load data with Pandas, plotting data with Seaborn <br /> - Define and explain the concept of correlation
5/10: Modeling Time-series Data | - Prior definition and Python functions for moving averages and autocorrelation <br /> - Prior exposure to linear regression with discussion of coefficients and residuals <br /> - pip install statsmodels (should be included with Anaconda)
5/15: Databases | There will be multiple ways to run the exercises: <br /> - Using Postgres Exercises <br /> - Setting up local Postgres <br /> - Install Postgres: if brew is installed, this should be as simple as brew install postgres <br /> - Using Wagon: Create an account at https://www.wagonhq.com/ and download the software
5/17: Next Steps with Data Science | - Define the data science workflow <br /> - Apply course information to your own professional interests
5/22: Neural Networks | - Understand Logistic Regression and link functions <br /> - Be familiar with training and testing classifiers and regressors
5/24: Final Presentations!!! |
