{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Section 9.0:_** Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statsmodels logistic regression is sm.Logit\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn import grid_search, cross_validation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Section 9.1_\n",
    "#### Guided Practice: Logit Function and Odds\n",
    "```diff\n",
    "+ The following section provides an opportunity for the student to create logit and sigmoid functions \"from scratch\" (see slides for formulas), as well as an example data set for converting odds into probability using those functions. The 2nd section focuses on visualizing our functions for greater illustration.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_func(odds):\n",
    "    # uses a float (odds) and returns back the log odds (logit)\n",
    "    return None\n",
    "\n",
    "def sigmoid_func(logit):\n",
    "    # uses a float (logit) and returns back the probability\n",
    "    return None\n",
    "\n",
    "odds_set = [\n",
    "    5./1,\n",
    "    20./1,\n",
    "    1.1/1,\n",
    "    1.8/1,\n",
    "    1.6/1\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sigmoid function (after defining it above)\n",
    "plt.axhline(0.5, color='grey')\n",
    "plt.axvline(0, color='grey')\n",
    "plt.plot(range(-6,6),[sigmoid_func(x) for x in range(-6,6)])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('probability')\n",
    "plt.title('inverse logit/sigmoidal function')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Section 9.2_\n",
    "#### Implement logistic regression with college admissions data\n",
    "```diff\n",
    "+ The following section provides an opportunity for the student to implement an existing sklearn model - LogisticRegression, and then examine its output.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = pd.read_csv('./dataset/collegeadmissions.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(pd.get_dummies(df['rank']))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LogisticRegression()\n",
    "X = df[['gre', 'gpa', 1, 2, 3,]]\n",
    "y = df['admit']\n",
    "\n",
    "lm.fit(X, y)\n",
    "\n",
    "predictions = lm.predict_proba(X)[:,1]\n",
    "\n",
    "print(lm.coef_)\n",
    "print(lm.intercept_)\n",
    "print(df.admit.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Section 9.3_\n",
    "#### Classification metrics: Confusion matrices, etc.\n",
    "```diff\n",
    "+ The following section provides an opportunity for the student to implement classification metrics, and present the results in a popular format - the ROC curve, which will be useful for working through the Titanic problem.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the ROC curve is based on various thresholds: it shows with a false positive rate (x-axis) ~0, it also expects a true positive rate (y-axis) ~0 (the same, ish, for the top right hand of the figure).\n",
    "\n",
    "This chart will help you compare models and determine where the decision line should exist for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y, predictions)\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_classes = (predictions > threshold).astype(int)\n",
    "\n",
    "print('accuracy: ' + str(accuracy_score(y, predicted_classes)))\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.title('varied thresholds')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(df['admit'], lm.predict(df[['gre', 'gpa', 1, 2, 3,]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(df['admit'], lm.predict(df[['gre', 'gpa', 1, 2, 3,]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Section 9.4_\n",
    "#### Independent Practice: Evaluating Logistic Regression with Alternative Metrics\n",
    "#### \"A _Titanic_ Problem\"\n",
    "```diff\n",
    "+ The following section provides a more in-depth opportunity to implement classification metrics using a more elaborate data set. The emphasis here should be on comparing alternative metrics and selecting the most appropriate one.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Goals **\n",
    "\n",
    "1. Spend a few minutes determining which data would be most important to use in the prediction problem. You may need to create new features based on the data available. Consider using a feature selection aide in sklearn. But a worst case scenario; identify one or two strong features that would be useful to include in the model.\n",
    "2. Spend 1-2 minutes considering which _metric_ makes the most sense to optimize. Accuracy? FPR or TPR? AUC? Given the business problem (understanding survival rate aboard the Titanic), why should you use this metric?\n",
    "3. Build a tuned Logistic model. Be prepared to explain your design (including regularization), metric, and feature set in predicting survival using the tools necessary (such as a fit chart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
